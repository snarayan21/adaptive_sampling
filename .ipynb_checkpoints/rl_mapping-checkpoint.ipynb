{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from perlin_noise import PerlinNoise\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition class: named tuple representing a single transition in the environment.\n",
    "# maps (s,a) pairs to (s', r) result. State here is (partly) observed map.\n",
    "# ReplayMemory: buffer of bounded size that holds recently observed transitions\n",
    "# should implement a .sample() method for selecting a random batch of transitions for training.\n",
    "\n",
    "# state consists of currently explored reward map, coordiantes of robot, current move, and total moves.\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # saves transition in replay memory\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment here is currently determininstic (?? is it?)\n",
    "# TODO: modify to contain expectations over transitions in environment. (non-deterministic)\n",
    "# maximize discounted cumulative reward (discount param gamma)\n",
    "# want neural net to be our Q function on (s,a). Input s, output Q(s,a) for all actions possible\n",
    "# policy pi defined by this neural net is pi(s) = argmax(a) Q(s,a)\n",
    "# update through temporal difference learning:\n",
    "# Q_pi(s,a) = r + gamma*(Q_pi(s', pi(s'))) = B\n",
    "# sum of expected reward from (s,a) --> r and the Q value of subsequent state s' and pi(s').\n",
    "# There should be an expectation here...expectation over all end states s', and transition probabilities.\n",
    "# temporal difference error: delta = Q(s,a) - (r + gamma*(max Q(s', a)))\n",
    "# Huber loss: MSE when error small, MAE when error large --> robust to outliers when we have noisy estimates of Q\n",
    "\n",
    "# model is a CNN that takes in the difference between the current and previous screen patches\n",
    "# two outputs: one for Q(s, left) and one for Q(s, right)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            # want to make sure the layer sizes change even with different input image sizes\n",
    "            return (size - (kernel_size-1) - 1) // stride + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "\n",
    "        linear_input_size = (convw*convh*32) + 4 # also inputting robot coordinates, current move, total moves\n",
    "        #self.fc1 = nn.Linear(linear_input_size, linear_input_size*2)\n",
    "        #self.fc2 = nn.Linear(linear_input_size*2, linear_input_size*2)\n",
    "        #self.fc3 = nn.Linear(linear_input_size*2, linear_input_size)\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        rmap, radditional = state\n",
    "        if rmap.dim() == 2:\n",
    "            rmap = torch.unsqueeze(rmap, 0)\n",
    "            rmap = torch.unsqueeze(rmap, 0)\n",
    "        if rmap.dim() == 3:\n",
    "            rmap = torch.unsqueese(rmap, 1)\n",
    "        if radditional.dim() == 1:\n",
    "            radditional = torch.unsqueeze(radditional, 0)\n",
    "        rmap = rmap.to(device)\n",
    "        radditional = radditional.to(device)\n",
    "        rmap = F.relu(self.bn1(self.conv1(rmap)))\n",
    "        rmap = F.relu(self.bn2(self.conv2(rmap)))\n",
    "        rmap = F.relu(self.bn3(self.conv3(rmap)))\n",
    "        x = torch.cat((rmap.view(rmap.size(0), -1), radditional.view(radditional.size(0), -1)), dim=1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9 # high exploration initially\n",
    "EPS_END = 0.05 # low exploration at end\n",
    "EPS_DECAY = 200 # controls epsilon decay rate\n",
    "TARGET_UPDATE = 10\n",
    "N_ACTIONS = 8\n",
    "N_MAPS = 10\n",
    "MAP_SIZE = 200\n",
    "OBS_SIZE = 20\n",
    "N_MOVES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(MAP_SIZE, MAP_SIZE, N_ACTIONS).to(device)\n",
    "target_net = DQN(MAP_SIZE, MAP_SIZE, N_ACTIONS).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# select_action function selects action according to epsilon greedy policy\n",
    "# parameter epsilon is the probability of exploring instead of exploiting.\n",
    "# plot_durations function is a helper for plotting durations of episodes\n",
    "# along with an average over last 100 episodes.\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    # sample from 0-1 uniform\n",
    "    sample = random.random()\n",
    "    # exponential decay of epsilon\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        # not within epsilon, so we exploit\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # within epsilon, explore. here we are returning random actions.\n",
    "        return torch.tensor([[random.randrange(N_ACTIONS)]], device=device, dtype=torch.long)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations(wipe=True):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.3)  # pause a bit so that plots are updated\n",
    "    if wipe:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize_model function performs sincle step of optimization.\n",
    "# samples batch, concatenates tensors, computes Q(s_t, a_t) and V(s_t+1) = max Q(s_t+1, a)\n",
    "# combines these into our loss. V(s) = 0 if terminal state.\n",
    "# target network computes V(s_t+1) for stability. Target network parameters frozen and are updated\n",
    "# with policy network's weights at regular intervals.\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch=Transition(*zip(*transitions))\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # final state is one where the current moves == total allowed moves\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = (torch.cat([s[0] for s in batch.next_state if s is not None]), torch.cat([s[1] for s in batch.next_state if s is not None]))\n",
    "\n",
    "    state_batch = (torch.cat([s[0] for s in batch.state]), torch.cat([s[1] for s in batch.state]))\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) -- model computes Q(s_t), then we select\n",
    "    # the columns of actions taken. These are actions which would have\n",
    "    # been taken for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_t+1) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # compute expected q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # compute Huber Loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        # in place version of clamp\n",
    "        # lol why are we doing this...\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map generation function\n",
    "def generate_map(size=1000):\n",
    "    oct1 = random.randint(1,4)\n",
    "    oct2 = random.randint(3,6)\n",
    "    oct3 = random.randint(5,8)\n",
    "    oct4 = random.randint(7,10)\n",
    "    noise1 = PerlinNoise(octaves=oct1)\n",
    "    noise2 = PerlinNoise(octaves=oct2)\n",
    "    noise3 = PerlinNoise(octaves=oct3)\n",
    "    noise4 = PerlinNoise(octaves=oct4)\n",
    "    xpix, ypix = size, size\n",
    "\n",
    "    pic = []\n",
    "    for i in range(xpix):\n",
    "        row = []\n",
    "        for j in range(ypix):\n",
    "            noise_val = noise1([i/xpix, j/ypix])\n",
    "            noise_val += 0.5 * noise2([i/xpix, j/ypix])\n",
    "            noise_val += 0.25 * noise3([i/xpix, j/ypix])\n",
    "            noise_val += 0.125 * noise4([i/xpix, j/ypix])\n",
    "\n",
    "            row.append(noise_val)\n",
    "        pic.append(row)\n",
    "\n",
    "    pic = np.array(pic)\n",
    "    # min-max normalize so that all values are between 0 and 1\n",
    "    pic = (pic - np.min(pic))/(np.max(pic) - np.min(pic))\n",
    "    \n",
    "    return pic\n",
    "\n",
    "def get_observation(truemap, obs_coords, obs_size=25):\n",
    "    row, col = obs_coords\n",
    "    observation = np.array(truemap)[row*obs_size:(row+1)*obs_size, col*obs_size:(col+1)*obs_size]\n",
    "    noise = np.random.uniform(high=np.mean(observation)/20, size=observation.shape)\n",
    "    noisy_observation = observation + noise\n",
    "    noisy_observation = (noisy_observation - np.min(noisy_observation))/(np.max(noisy_observation)-np.min(noisy_observation))\n",
    "    location = (row*obs_size,(row+1)*obs_size,col*obs_size,(col+1)*obs_size)\n",
    "    # simple reward: sum of pixels in observation\n",
    "    reward = np.sum(observation)\n",
    "    \n",
    "    return observation, noisy_observation, location, reward\n",
    "\n",
    "def action_to_location(currloc, action, obs_size, map_size):\n",
    "    # Actions are:\n",
    "    # 7  0  1\n",
    "    # 6  x  2\n",
    "    # 5  4  3\n",
    "    obs_locs = map_size // obs_size\n",
    "    oldr, oldc = currloc\n",
    "    newr, newc = currloc\n",
    "    if action == 0:\n",
    "        newr = oldr - 1 if oldr > 0 else oldr\n",
    "        newc = oldc\n",
    "    if action == 1:\n",
    "        newr = oldr - 1 if oldr > 0 else oldr\n",
    "        newc = oldc + 1 if newc < obs_locs-1 else oldc\n",
    "    if action == 2:\n",
    "        newr = oldr\n",
    "        newc = oldc + 1 if newc < obs_locs-1 else oldc\n",
    "    if action == 3:\n",
    "        newr = oldr + 1 if newr < obs_locs-1 else oldr\n",
    "        newc = oldc + 1 if newc < obs_locs-1 else oldc\n",
    "    if action == 4:\n",
    "        newr = oldr + 1 if newr < obs_locs-1 else oldr\n",
    "        newc = oldc\n",
    "    if action == 5:\n",
    "        newr = oldr + 1 if newr < obs_locs-1 else oldr\n",
    "        newc = oldc - 1 if oldc > 0 else oldc\n",
    "    if action == 6:\n",
    "        newr = oldr\n",
    "        newc = oldc - 1 if oldc > 0 else oldc\n",
    "    if action == 7:\n",
    "        newr = oldr - 1 if oldr > 0 else oldr\n",
    "        newc = oldc - 1 if oldc > 0 else oldc\n",
    "    return (newr, newc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate different maps.\n",
    "all_maps = [generate_map(size=MAP_SIZE) for i in range(N_MAPS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for i_map in range(N_MAPS):\n",
    "    # initialize environment and state\n",
    "    if i_map % 5 == 0:\n",
    "        print(\"Episode\", i_map)\n",
    "    # get the initial map for this iteration from the all_maps list\n",
    "    true_map = all_maps[i_map]\n",
    "    # select a random starting position for the robot\n",
    "    curr_row = random.randint(0,(MAP_SIZE//OBS_SIZE)-1)\n",
    "    curr_col = random.randint(0,(MAP_SIZE//OBS_SIZE)-1)\n",
    "    curr_moves = 0\n",
    "    curr_observed_map = np.zeros((MAP_SIZE, MAP_SIZE))\n",
    "    # get observation at initial location\n",
    "    true_obs, noisy_obs, obsloc, reward = get_observation(true_map, (curr_row, curr_col), obs_size=OBS_SIZE)\n",
    "    # use the true observation for now. Will use noisy later.\n",
    "    curr_observed_map[obsloc[0]:obsloc[1], obsloc[2]:obsloc[3]] = np.copy(true_obs)\n",
    "    next_observed_map = np.copy(curr_observed_map)\n",
    "    # additional robot information in state: current location, curr moves, total moves\n",
    "    curr_r_additional = np.array([curr_row, curr_col, curr_moves, N_MOVES])\n",
    "    next_r_additional = np.copy(curr_r_additional)\n",
    "    #state is a tuple of currently observed map and additional robot info\n",
    "    curr_state = (curr_observed_map, curr_r_additional)\n",
    "    done = False\n",
    "    for m in range(1,N_MOVES+1):\n",
    "        torch_curr_observed_map = torch.FloatTensor(np.copy(curr_state[0]))\n",
    "        torch_curr_r_additional = torch.FloatTensor(np.copy(curr_state[1]))\n",
    "        torch_curr_state = (torch_curr_observed_map, torch_curr_r_additional)\n",
    "        # select and perform action. Actions are:\n",
    "        # 7  0  1\n",
    "        # 6  x  2\n",
    "        # 5  4  3\n",
    "        action = select_action(torch_curr_state)\n",
    "        # retrieve new location based on action\n",
    "        next_row, next_col = action_to_location((curr_r_additional[0], curr_r_additional[1]), action, OBS_SIZE, MAP_SIZE)\n",
    "        # we've made an additional move\n",
    "        next_moves = curr_r_additional[2] + 1\n",
    "        # check if we have made maximum moves. set done accordingly\n",
    "        if next_moves == N_MOVES:\n",
    "            done = True\n",
    "        # observe at new location\n",
    "        next_true_obs, next_noisy_obs, next_obsloc, next_reward = get_observation(true_map, (next_row, next_col), obs_size=OBS_SIZE)\n",
    "        # so they are receiving some type of reward from environment here.\n",
    "        # need to decide how this function will operate\n",
    "        reward = torch.FloatTensor([reward])\n",
    "\n",
    "        # construct new state\n",
    "        if not done:\n",
    "            next_observed_map[next_obsloc[0]:next_obsloc[1], next_obsloc[2]:next_obsloc[3]] = np.copy(next_true_obs)\n",
    "            next_r_additional = np.array([next_row, next_col, next_moves, N_MOVES])\n",
    "            torch_next_observed_map = torch.FloatTensor(next_observed_map)\n",
    "            torch_next_r_additional = torch.FloatTensor(next_r_additional)\n",
    "            torch_next_state = (torch_next_observed_map, torch_next_r_additional)\n",
    "            next_state = (next_observed_map, next_r_additional)\n",
    "        else:\n",
    "            # end state reached, moves exhausted\n",
    "            next_state = None\n",
    "        \n",
    "        # store transition in memory\n",
    "        memory.push(torch_curr_state, action, torch_next_state, reward)\n",
    "\n",
    "        # move curr state to next state\n",
    "        curr_state = next_state\n",
    "\n",
    "        # perform one step of optimization (only on policy network rn)\n",
    "        # optimization only occurs when len of replay buffer >= batch size.\n",
    "        # (was worried about that.)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # update target network by copying from policy net\n",
    "        if m % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print(\"Complete\")\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "84fc1219d7a8f6c286df3f59ccd0b6a41191ea48d293cb5d19ccee77aa4c0389"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
